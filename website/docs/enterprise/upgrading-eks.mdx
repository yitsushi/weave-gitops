---
title: Upgrading with EKS
hide_title: true
sidebar_position: 1
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import TierLabel from "../_components/TierLabel";
import CodeBlock from "@theme/CodeBlock";
import BrowserOnly from "@docusaurus/BrowserOnly";

# Upgrading with EKS <TierLabel tiers="enterprise" />

## How to: Upgrade to Weave GitOps Enterprise

:::note BEFORE YOU START

Make sure the following software is installed before continuing with these instructions:

- `gitops` >= 0.5.0-rc2. Use `gitops version` to check the currently installed version. If needed, download a newer version of Weave GitOps from the [releases page](https://github.com/weaveworks/weave-gitops/releases).
- `github cli` >= 2.3.0 [(source)](https://cli.github.com/)
- `kubectl` [(source)](https://kubernetes.io/docs/tasks/tools/#kubectl)
- `eksctl` [(source)](https://github.com/weaveworks/eksctl/releases)
- `aws cli` [(source)](https://aws.amazon.com/cli/)
- `gh cli` [(source)](https://github.com/cli/cli)
- `clusterclt` >= v1.0.1 [(source)](https://github.com/kubernetes-sigs/cluster-api/releases)
- `clusterawsadm` >= v1.1.0 [(source)](https://github.com/kubernetes-sigs/cluster-api-provider-aws/releases)

The `AWS_ACCESS_KEY_ID`and `AWS_SECRET_ACCESS_KEY` of a user should be configured either via `aws configure` or exported in the current shell.
The `GITHUB_TOKEN` should be set as an environment variable in the current shell. It should have permissions to create Pull Requests against the cluster config repo.
:::

Upgrading requires we:

1. Have a cluster with Weave GitOps installed
2. Apply the entitlements secret ( contact Weaveworks sales for this )
3. Install Weave Gitops Enterprise
4. Check that Weave GitOps Enterprise has been installed
5. Install a CAPI provider
6. Connect the management cluster up to itself

### 1. Install `gitops` on a new cluster

We'll create a new EKS Cluster here as an example.

#### 1.1 Prepare IAM for installation

The Cluster API needs special permissions in AWS. Use the `clusterawsadm` command below to roll out a CloudStack to installs the permissions into your AWS account. While the CloudStack is bound to a region, the resulting permissions are globally scoped. You can use any AWS Region that you have access to. The `clusterawsadm` command takes an AWSIAMConfiguration file. We have provided a working example for you :

```yaml title="eks-config.yaml"
apiVersion: bootstrap.aws.infrastructure.cluster.x-k8s.io/v1beta1
kind: AWSIAMConfiguration
spec:
  bootstrapUser:
    enable: true
  eks:
    iamRoleCreation: false # Set to true if you plan to use the EKSEnableIAM feature flag to enable automatic creation of IAM roles
    defaultControlPlaneRole:
      disable: false # Set to false to enable creation of the default control plane role
    managedMachinePool:
      disable: false # Set to false to enable creation of the default node pool role
```

Run `clusterawsadm` command to create the IAM group.

```bash
$ clusterawsadm bootstrap iam create-cloudformation-stack --config eks-config.yaml --region $REGION
```

Create an IAM User. This user will be used as a kind of service account. Assign the newly created group to this user. The group name will be something like: `cluster-api-provider-aws-s-AWSIAMGroupBootstrapper-XXXX`. Create a secret for the newly created IAM user.

#### 1.2 Create the cluster

In testing we used the following values
`$INSTANCESIZE` : t3.large
`$NUMOFNODES` : 2
`$MINNODES` : 2
`$MAXNODES` : 6

```bash
eksctl create cluster -n "$CLUSTERNAME" -r "$REGION" --nodegroup-name workers -t $INSTANCESIZE --nodes $NUMOFNODES --nodes-min $MINNODES --nodes-max $MAXNODES --ssh-access --alb-ingress-access
```

#### 1.3 Add cluster to kubeconfig

Once the cluster is created, add the cluster to your `kubeconfig`

```bash
aws eks --region "$REGION" update-kubeconfig --name "$CLUSTERNAME"
```

#### 1.4 Create a new Git repo

```bash
gh repo create my-management-cluster --private --confirm
cd my-management-cluster
echo "# my-management-cluster" > README.md
git add README.md
git commit --all --message "init commit"
git push -u origin main
```

#### 1.5 Install Weave Gitops

```bash
gitops install --config-repo https://github.com/my-org/my-management-cluster
```

### 2. Apply the entitlements secret

Contact sales@weave.works for a valid entitlements secret. Then apply it to the cluster:

```bash
kubectl apply -f entitlements.yaml
```

### 3. Upgrade

Run the upgrade command specifying the same `--config-repo` used in install and the desired version of Weave GitOps Enterprise.

```bash
gitops upgrade --version 0.0.16
```

:::note
You may need to add the `--base` flag to the above command to indicate your local branch name, if it is not set to `main`.
:::

A **Pull Request** will be created against your cluster repository. **Review and merge** this pull request to upgrade to Weave GitOps Enterprise.

### 4. Checking that WGE is installed

You should now be able to load the WGE UI:

```bash
kubectl port-forward --namespace wego-system deployments.apps/weave-gitops-enterprise-nginx-ingress-controller 8000:80
```

The WGE UI should now be accessible at [http://localhost:8000](http://localhost:8000).

### 5. Install a CAPI provider

:::note `clusterctl` versions

The example templates provided in this guide have been tested with `clusterctl` version `1.0.1`. However you might need to use an older or newer version depending on the capi-providers you plan on using.

Download a specific version of clusterctl from the [releases page](https://github.com/kubernetes-sigs/cluster-api/releases).
:::

In order to be able to provision Kubernetes clusters, a CAPI provider needs to be installed.

```
clusterctl init --infrastructure aws
```

### 6. Connect the management cluster up to itself

_Connecting a cluster_ installs the agent which is responsible for detecting new clusters and reporting their status to the UI. We need to install the agent on our newly created management cluster. Check out [How to: Connect a cluster](../cluster-management/managing-existing-clusters.mdx#how-to-connect-a-cluster). The agent should be loaded onto our new management cluster, give it a name like **Management** and leave the Ingress URL blank.

Head over to [Getting started](../enterprise/deploying-capa.mdx) to create your first CAPI Cluster.
